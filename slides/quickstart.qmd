---
title: "LLM Quick Start"
format:
  revealjs:
    theme: simple
    transition: slide
    slide-number: true
    chalkboard: true
editor:
  render-on-save: true
---

```{r include=FALSE}
slack_channel <- "#hackathon-09"
```

# Setup {.smaller}

- Clone https://github.com/jcheng5/llm-quickstart
- Grab your OpenAI API key; see the thread in <code>`r slack_channel`</code>

- For R
  - `pak::pak(c("tidyverse/elmer", "jcheng5/shinychat", "dotenv"))`
- For Python
  - `pip install -r requirements.txt`

# Introduction

## Framing LLMs

::: {.incremental}
- Our focus: Practical, actionable information
  - Often, _just_ enough knowledge so you know what to search for (or better yet, what to ask an LLM)
- We will treat LLMs as black boxes
- Don't focus on how they work (yet)
  - Leads to bad intuition about their capabilities
  - Better to start with a highly empirical approach
:::

# Anatomy of a Conversation

## LLM Conversations are HTTP Requests

::: {.incremental}
- Each interaction is a separate HTTP API request
- The API server is entirely stateless (despite conversations being inherently stateful!)
:::

## Example Conversation

::: {style="text-align: right;"}
"What's the capital of the moon?"
:::

`"There isn't one."`

::: {style="text-align: right;"}
"Are you sure?"
:::

`"Yes, I am sure."`

## Example Request

```{.bash code-line-numbers="|5|6-9|7|8"}
curl https://api.openai.com/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -d '{
    "model": "gpt-4o",
    "messages": [
        {"role": "system", "content": "You are a terse assistant."},
        {"role": "user", "content": "What is the capital of the moon?"}
    ]
}'
```

- System prompt: behind-the-scenes instructions and information for the model
- User prompt: a question or statement for the model to respond to

## Example Response (abridged)

```{.json code-line-numbers="|3-6|7|12"}
{
  "choices": [{
    "message": {
      "role": "assistant",
      "content": "The moon does not have a capital. It is not inhabited or governed.",
    },
    "finish_reason": "stop"
  }],
  "usage": {
    "prompt_tokens": 9,
    "completion_tokens": 12,
    "total_tokens": 21,
    "completion_tokens_details": {
      "reasoning_tokens": 0
    }
  }
}
```

## Example Request

```{.bash code-line-numbers="|9|10"}
curl https://api.openai.com/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -d '{
    "model": "gpt-4o",
    "messages": [
      {"role": "system", "content": "You are a terse assistant."},
      {"role": "user", "content": "What is the capital of the moon?"},
      {"role": "assistant", "content": "The moon does not have a capital. It is not inhabited or governed."},
      {"role": "user", "content": "Are you sure?"}
    ]
}'
```

## Example Response (abridged)

```{.json code-line-numbers="|3-6|10-12"}
{
  "choices": [{
    "message": {
      "role": "assistant",
      "content": "Yes, I am sure. The moon has no capital or formal governance."
    },
    "finish_reason": "stop"
  }],
  "usage": {
    "prompt_tokens": 52,
    "completion_tokens": 15,
    "total_tokens": 67,
    "completion_tokens_details": {
      "reasoning_tokens": 0
    }
  }
}
```

## Tokens

::: {.incremental}
- Fundamental units of information for LLMs
- Words, parts of words, or individual characters
  - "hello" → 1 token
  - "unconventional" → 3 tokens: `un|con|ventional`
  - 4K video frame at full res → 6885 tokens
- Important for:
  - Model input/output limits
  - API pricing is usually by token (see [calculator](https://gptforwork.com/tools/openai-chatgpt-api-pricing-calculator))
:::

# Choose a Package {.smaller}

- R:
  - `elmer` high-level, easy, much less ambitious than langchain
    - OpenAI, Anthropic, Google are well supported
    - Several other providers are supported but may not be as well tested
    - Get help from [Elmer Assistant](https://jcheng.shinyapps.io/elmer-assistant/)
- Python:
  - `openai` - low-level, but solid
  - `langchain` - high-level, all models, sprawling scope... but polarizing architecture, steep learning curve, and supposedly questionable code quality
    - Evolved from v0.1, v0.2, and now `langgraph`
  - `chatlas` - high-level, basically a port of R's `elmer`
  - Many, many other options are available

# Your Turn

## Instructions

Open and run one of these options:

- `01-basics.R`
- `01-basics-openai.py` (low level library)
- `01-basics-langchain.py` (high level framework)
- `01-basics-chatlas.py` (high level framework, similar to R)

If it errors, now is the time to debug.

If it works, study the code and try to understand how it maps to the low-level HTTP descriptions we just went through.

## Summary

- A message is an object with a `role` ("system", "user", "assistant") and a `content` string
- A chat conversation is a growing list of messages
- The OpenAI chat API is a stateless HTTP endpoint: takes a list of messages as input, returns a new message as output

# Creating chatbot UIs

## Shiny for R

{`shinychat`} package\
  [https://github.com/jcheng5/shinychat](https://github.com/jcheng5/shinychat)

- Designed to be used with elmer
- [Elmer Assistant](https://jcheng.shinyapps.io/elmer-assistant) is quite good for getting started

## Shiny for Python

Use the [official `ui.Chat` component](https://shiny.posit.co/py/components/display-messages/chat/) for Shiny for Python

- Ask [Shiny assistant](https://gallery.shinyapps.io/assistant/) for help (ping me or Winston if you need us to approve your invite)
- Do NOT use the older ChatStream component

## Other Python frameworks

- Streamlit has an excellent [chat component](https://docs.streamlit.io/develop/tutorials/llms/build-conversational-apps) with a [nice LangChain integration](https://docs.streamlit.io/develop/tutorials/llms/llm-quickstart)
- Gradio has a [chat component](https://www.gradio.app/guides/creating-a-chatbot-fast) that is extremely easy to use

# Tool Calling

## What is Tool Calling?

::: {.incremental}

- Allows LLMs to interact with other systems
- Sounds complicated? It isn't!
- Supported by most of the newest LLMs, but not all (notably, not the new OpenAI `o1` models, yet)

:::

## How It Works

::: {.incremental}
- [Not like this](https://sequencediagram.org/index.html?presentationMode=readOnly#initialData=C4S2BsFMAIAkHsDu1j3uaBjAhucIA7Ac2gBEB5AUQGUA5QTAIAVaReAJwGsAoL7TVNtACqAZ0hsuAW2zBxIXABNIIkEQIhM8AiOgAxAIwA2AOwBBaKZEqRwbAWBcADtjahMIZ-ejxHkAokgZAAtxADp2Ih5RcQBaAD5La1t7AC5oAG8AHQJoaDZ0SDSAIgBXMTYigBps3M17P2BigHUgmQByHWAQlkCu8TzVIOBoAiRoGT0-RGwAT2gABRcOAH4q7IBfLkSQGztgeJ8p3pC2cLYiNIBxSmYAegDg8VvMErY2Btuy24AGACZf-QAVlCACsRFouId-McwhF4ttdqkMkVZJJHEU0oZAZVoEU6gowCAtCIMbiRCUCAQZkVNgjkvs4tE2Gksjk8gVitgrDt6Ws2XVZEiigBJYAdaBY6BKIjvZTjAgKaDkykzQAoBNhoAAjQIlUAAMxKGAUswAhEUNkA) - with the assistant executing stuff
- [Yes like this](https://sequencediagram.org/index.html?presentationMode=readOnly#initialData=C4S2BsFMAIAkHsDu1j3uaBjAhucIA7Ac2gBEB5AUQGVpF4AnAawCgWAHbB0TETg4NHjtIBRJGzAAFpAYA6RkRbZMqBtACqAZ1ksAtpNkhcAE0haQRAiEzwCW6ADEAjADYA7AEFonrRa3A2AJs2rIAtAB8vv6BAgBc0ADeADoE0NAM6JAJAEQArjoMOQA0qem2AqLAuQDqUpIA5A7SMOKSMuoMllKCBEjQkk6iiNgAntAAClxMAPwlZSho4FoJANpEkMAA+ph5DAxVW20tDAC6qQC+LNEgAUHAkaEMCQDCngAy79Ab27v7h8cOgAKHIABgATODnABWHIAShYT0iwmGEhOCgYRASAHFKAAVaAAekBskJfwOAkJBUJEKh0LkACstHYWCixGiOhiiI9CglEjlgJA9OwcglXNDitAchUTGAQHYtKKpVo8gQCKMclckVE-LdYtVoAAlfEaQ0AOSSAqFIrFEqlMrlCqVORVao1VxudwEPNkfIWmSguWwuq9wHmaSwdkF8SlAElgE1oOLoGYiAdzAMCCZoK71YAUAmw0AARhI8qAAGZ5DAmMYAQhylyAA)
  - User asks assistant a question; includes metadata for available tools
  - Assistant asks the user to invoke a tool, passing its desired arguments
  - User invokes the tool, and returns the output to the assistant
  - Assistant incorporates the tool's output as additional context for formulating a response
:::

## How It Works

Another way to think of it:

- The client can perform tasks that the assistant can't do
- Tools put control into the hands of the assistant—it decides when to use them, and what arguments to pass in, and what to do with the results
- Having an "intelligent-ish" coordinator of tools is a surprisingly general, powerful capability!

# Your Turn {.smaller}

Take a minute to look at _one of_ the following docs. See if you can get them to run, and try to understand the code.

- R: [`elmer` docs](https://elmer.tidyverse.org/articles/tool-calling.html) (anticlimactically easy), or example `02-tools.R` in `llm-quickstart` repo
- Python
  - `openai` example: `02-tools-openai.py` (tedious, low-level, but understandable)
  - `langchain` example: `02-tools-langchain.py` (not bad)
  - `chatlas` example: `02-tools-chatlas.py` (easy, like `elmer`)


# Choosing a model

- OpenAI ChatGPT
- Anthropic Claude
- Google Gemini
- Meta Llama (Can run locally, or access via API)

## OpenAI models

- **GPT-4o**: best general purpose model
- **GPT-4o-mini**: similar to 4o, but faster and cheaper (and dumber)
- **o1-preview**: uses chain of thought (available via API only for high usage tiers)
- **o1-mini**

## Anthropic models

- **Claude 3.5 Sonnet**: best model for code generation
- **Claude 3.5 Haiku**: faster, cheaper (and dumber) than Sonnet

## Llama models

- Open weights: you can download the model
- Can run locally, for example with [Ollama](https://ollama.com/)
- **Llama 3.1 405b**: text, 229GB
- **Llama 3.2 90b**: text+vision, 55GB
- **Llama 3.2 11b**: text+vision, 7.9GB (can run comfortably on Macbook Pro)
- **Llama 3.2 3b**: text, 2GB
- **Llama 3.2 1b**: text, 0.8GB
- Can also access these models via API with [Openrouter](https://openrouter.ai/), [Groq](https://groq.com/), [Hugging Face](https://huggingface.co/docs/api-inference/en/index)


# Customizing behavior and adding knowledge

- **The problem**
  - You want to customize how the LLM responds
  - LLM doesn't know your specific information
- **Some solutions**
  - Prompt engineering
  - Retrieval-Augmented Generation
  - Fine tuning

## Prompt engineering: Directing behavior/output

- "Respond with just the minimal information necessary."
- "Explain your response in detail."
- "Think through this step-by-step."
- "Carefully read and follow these instructions..."
- "If the user asks a question related to data processing, produce R code to accomplish that task."
- "Be careful to only provide answers that you are sure about. If you are uncertain about an answer, say so."


## Prompt engineering: Using examples to guide behavior

- **Goal:** Extract ingredient list from recipe and return in a structured format.

- Example user input:

  ```
  In a large bowl, cream together 1 cup of softened unsalted butter and ½ cup of white sugar until smooth. Beat in 1 egg and 1 teaspoon of vanilla extract. Gradually stir in 2 cups of all-purpose flour until the dough forms. Finally, fold in 1 cup of semisweet chocolate chips. Drop spoonfuls of dough onto an ungreased baking sheet and bake at 350°F (175°C) for 10-12 minutes, or until the edges are lightly browned. Let the cookies cool on the baking sheet for a few minutes before transferring to a wire rack to cool completely. Enjoy!
  ```


## System prompt 1: just instructions

```
The user input contains a recipe. Extract a list of ingredients and return it in JSON format.
```

Assistant response:

```{.json}
{
  "ingredients": [
    "1 cup of softened unsalted butter",
    "½ cup of white sugar",
    "1 egg",
    "1 teaspoon of vanilla extract",
    "2 cups of all-purpose flour",
    "1 cup of semisweet chocolate chips"
  ]
}
```

## System prompt 2: detailed description of output

```
The user input contains a recipe. Extract a list of ingredients and return it in JSON format. It should be an array of objects, where each object has keys `ingredient`, `quantity`, and `unit`. Put each object on one line of output.
```

Assistant response:

```{.json}
[
    {"ingredient": "unsalted butter", "quantity": 1, "unit": "cup"},
    {"ingredient": "white sugar", "quantity": 1/2, "unit": "cup"},
    {"ingredient": "egg", "quantity": 1, "unit": "large"},
    {"ingredient": "vanilla extract", "quantity": 1, "unit": "teaspoon"},
    {"ingredient": "all-purpose flour", "quantity": 2, "unit": "cups"},
    {"ingredient": "semisweet chocolate chips", "quantity": 1, "unit": "cup"}
]
```

## System prompt 3: one-shot example

````
The user input contains a recipe. Extract a list of ingredients and return it in JSON format.

Example Output:

```json
[
    { "ingredient": "Flour", "quantity": 1, "unit": "cup" },
    { "ingredient": "Vegetable oil", "quantity": 0.5, "unit": "tsp" },
    { "ingredient": "Onion", "quantity": 1, "unit": null },
]
```
````

Assistant response:

```{.json}
[
    { "ingredient": "Unsalted butter", "quantity": 1, "unit": "cup" },
    { "ingredient": "White sugar", "quantity": 0.5, "unit": "cup" },
    { "ingredient": "Egg", "quantity": 1, "unit": null },
    { "ingredient": "Vanilla extract", "quantity": 1, "unit": "teaspoon" },
    { "ingredient": "All-purpose flour", "quantity": 2, "unit": "cups" },
    { "ingredient": "Semisweet chocolate chips", "quantity": 1, "unit": "cup" }
]
```

## Adding context/knowledge to prompt

- Add documentation files to prompt
- Add positive examples (negative examples don't work well)
- Docs must fit in context window
- Examples
  - [Elmer assistant](https://github.com/jcheng5/elmer-assistant/blob/main/prompt.generated.md) uses README files in prompt
  - [Sidebot](https://github.com/jcheng5/py-sidebot/blob/main/prompt.md)
  - [FastHTML LLM prompt](https://docs.fastht.ml/llms-ctx.txt)

## RAG: Retrieval Augmented Generation

- Useful when documents don't fit into context window
- Steps:
  - User sends query to app: _"How do I ...?"_
  - App **retrieves** relevant chunks of text via search
  - App sends text and query to LLM
    - _<chunk 1>, <chunk 2>, <chunk 3>. How do I ...?_
  - LLM responds with answer
- Search method typically a semantic instead of keyword search, using vector DB
- LLM will only know about chunks that were retrieved; does not "know" entire corpus
- In general, prompt engineering works better, if docs fit in context window

## Fine tuning

- Update weights for an existing model with new information
- Not all models can be fine-tuned
- Data must be provided in chat conversation format, with query and response
  - Can't just feed it documents -- this makes fine-tuning more difficult in practice
- Supposedly not very effective unless you have a lot of training data

##

![](prompt-rag-finetune.png)


## More on prompting, RAG, fine tuning

- Try prompting first, before RAG and fine tuning.
- OpenAI's [prompt engineering guide](https://platform.openai.com/docs/guides/prompt-engineering)
- [Fine-tuning vs. RAG article](https://finetunedb.com/blog/fine-tuning-vs-rag/)

## Using posit.cloud

- [Posit.cloud workspace](https://posit.cloud/spaces/564193/join?access_code=HFgo5wTL-yjjlV4v4bLbh9Hm9g6-0eiAqRbms4Sa) with some examples
  - Contains examples from [https://github.com/jcheng5/llm-quickstart](https://github.com/jcheng5/llm-quickstart)

# Getting structured output

## Going beyond chat

- **Structured output** can be easily consumed by code: JSON, YAML, CSV, etc.
- **Unstructured output** cannot: text, images, etc.

LLMs are good at generating unstructured output, but with a little effort, you can get structured output as well.

## Several techniques (choose one) {.smaller}

- **Post-processing:** Use a regular expression to extract structured data from the unstructured output (e.g. <code>/&grave;&grave;&grave;json\\n(.*?)\\n&grave;&grave;&grave;/</code>)
- **System prompt:** Simply ask the LLM to output structured data. Be clear about what specific format you want, and provide examples---it _really_ helps!
- **Structured Output:** GPT-4o and GPT-4o-mini now have a first-class Structured Output feature: outputs strictly adhere to a JSON schema you write. (Docs: [openai](https://platform.openai.com/docs/guides/structured-outputs), [LangChain](https://python.langchain.com/docs/how_to/structured_output/))
- **Tool calling:** Create a tool to receive your output, e.g., `set_result(object)`, where its implementation sets some variable. (Works great for elmer.)
- **LangChain:** Has its own [abstractions](https://python.langchain.com/docs/how_to/#output-parsers) for parsing unstructured output

Ask <code>`r slack_channel`</code> for help if you're stuck! (Or ask ChatGPT/Claude to make an example.)

# Vision

## Using images as input

- Modern models are pretty good at this
- Can understand both photographs and plots
- Examples for R and Chatlas are in your repo as `05-vision*`
- See docs for [LangChain multimodal](https://python.langchain.com/docs/how_to/multimodal_inputs/), [OpenAI vision](https://platform.openai.com/docs/guides/vision)

# Brainstorming

## Past cohort projects

- [Cohort 1](https://docs.google.com/presentation/d/1wJKis5xUvce2PS2ZeUOp3h-L6Ul9NQ2CgNeZ9LsF5W8/edit?usp=sharing)
- [Cohort 2](https://docs.google.com/presentation/d/1j15NfbrRDfHIbnO7Jt3gS63CafArboe70AUJw4hFn8E/edit?usp=sharing)
- [Cohort 3](https://docs.google.com/presentation/d/1NmgeTgzs31r8g8-b2liOzKXDmYU_iXSIA8Ih8Z4ZzMM/edit?usp=sharing)
- [Cohort 4](https://docs.google.com/presentation/d/157q63jIrrTLHwO5SSN9RINs5FxN-9o1YcffKnefs_wg/edit#slide=id.p)
- [Cohort 5](https://docs.google.com/presentation/d/1taHrsRcjHcFI0bTXEZjP3fNs5beiDkaiKTcMHonWGfQ/edit#slide=id.p)
- [Cohort 6](https://docs.google.com/presentation/d/1e7oGe_I6yp69m111b5BjGnguo7AQRua6pXBS3WaP7FY/edit?usp=sharing)
- [Cohort 7](https://docs.google.com/presentation/d/1DZVC7d-ycbnwh6nbDyNZEGwdtPMD3FB4NoIf0dd915o/edit?usp=sharing)

## Interesting tools that don't require coding

- [Google NotebookLM](https://notebooklm.google/)
- [Claude Projects](https://claude.ai/projects) (requires Claude subscription)
